Experiment

1.1 No Physics loss with 700 epochs with training data from whole domain with 20 samples
1.2 Physics loss with 700 epochs with traing data from whole domain with 20 samples

Remarks: Seems like both have similar performance as it seems like there is enough data to capture the
complexity of the target function

1.3 No physics loss with 700 epochs with training data from whole domain with 10 samples
1.4 Physics loss with 700 epochs with traing data from whole domain with 10 samples

Remarks: Looks like the data is fit better with the added physics loss regularization as the data is
becoming more sparse

1.5 No physics loss with 700 epochs with training data from whole domain with 5 samples
1.6 Physics loss with 700 epochs with training data from whole domain with 5 samples (regularization of physics
loss with weight 0.01 rather than the earlier 0.0001)

Remarks: It seems like the physics loss can greatly improve the learning when the weight is increased,
in earlier experiments it seemed when the weight was too large that there would be underfitting as the
physics loss would prevent the model from fitting the data, however, with less data, it seems like this is less
of a factor and the regularization helps fit the data better

Comments on logistic regression experiments: We see that the activation function is pivotal when training PINNs.
When we first were developing the algorithm, we used the tanh activation function which led to the results below with
physics loss included

Experiment 1.7: In this experiment we used the tanh activation function and as can be seen here we have that the
model is not capable of learning. We believe this might be because the model gets stuck at a local optimum. We also think
that this might be because of the phenomenon of vanishing gradients problem. This occurs when the gradient becomes extremely
small in magnitude and thus the update does almost nothing.

Other comments: From this case we see that the model is much harder to build and also much slower than classical methods
for solving the same problem. However, one advantage of this PINNs method is that once the model is trained you are able to
recalculate the predicted value for any time value you want. In contrast, with traditional methods, in order to get the value of a point
that is not on the mesh you initially set up, you need to make an entirely new mesh and start over. However, it seems like in order to actually used
this PINNs method in practice you would want to solve a much more complex problem which is harder for traditional methods to solve. However, as this is
only a pedagogical project we stick to relatively simple ODEs.


Next, we generalize to a more complex nonlinear ODE which is a generalization of the 